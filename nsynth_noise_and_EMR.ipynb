{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoba/eVrng7hrO4iqbcNmK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fireHedgehog/music-intrument-OvA-model/blob/main/nsynth_noise_and_EMR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDtzSlMnnrZN"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Load Pretrained Models\n",
        "Assuming the models are saved in a specific directory within your Google Drive, you can create a model cache like this:"
      ],
      "metadata": {
        "id": "uAs01mGMoBv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model_dir = '/content/drive/My Drive/path_to_your_models'  # Update this path\n",
        "model_cache = {}\n",
        "\n",
        "label_map = {\n",
        "    0: 'bass', 1: 'brass', 2: 'flute', 3: 'guitar',\n",
        "    4: 'keyboard', 5: 'mallet', 6: 'organ', 7: 'reed',\n",
        "    8: 'string', 10: 'vocal'\n",
        "}\n",
        "\n",
        "for label, instrument_name in label_map.items():\n",
        "    model_path = os.path.join(model_dir, f'{instrument_name}_classifier.h5')\n",
        "    if os.path.exists(model_path):\n",
        "        model_cache[instrument_name] = load_model(model_path)\n",
        "    else:\n",
        "        print(f\"Model for {instrument_name} not found.\")\n",
        "\n",
        "\n",
        "def audio_to_spectrogram(audio_sample, sr=16000, n_fft=2048, hop_length=512):\n",
        "    \"\"\"Convert audio to spectrogram.\"\"\"\n",
        "    spectrogram = librosa.stft(audio_sample, n_fft=n_fft, hop_length=hop_length)\n",
        "    spectrogram_db = librosa.amplitude_to_db(abs(spectrogram))\n",
        "    return spectrogram_db\n"
      ],
      "metadata": {
        "id": "74m755G_thX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Prepare Dataset Samples\n"
      ],
      "metadata": {
        "id": "_ZkfZuAnoJNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "def get_samples_for_each_family(n_samples=100):\n",
        "    datasets = {\n",
        "        'train': tfds.load('nsynth/gansynth_subset', split='train', shuffle_files=True),\n",
        "        'valid': tfds.load('nsynth/gansynth_subset', split='validation', shuffle_files=True),\n",
        "        'test': tfds.load('nsynth/gansynth_subset', split='test', shuffle_files=True)\n",
        "    }\n",
        "    samples, labels = [], []\n",
        "\n",
        "    for family_id, instrument_name in label_map.items():\n",
        "        count = 0\n",
        "        for split in ['valid', 'test', 'train']:\n",
        "            if count >= n_samples:\n",
        "                break\n",
        "            for example in tfds.as_numpy(datasets[split]):\n",
        "                if example['instrument']['family'] == family_id:\n",
        "                    samples.append(example['audio'])\n",
        "                    labels.append(family_id)\n",
        "                    count += 1\n",
        "                    if count >= n_samples:\n",
        "                        break\n",
        "\n",
        "    return samples, labels\n"
      ],
      "metadata": {
        "id": "SUUlMsfooOV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Predict and Analyze\n"
      ],
      "metadata": {
        "id": "V2jdwNjMoULG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_analyze(samples, labels):\n",
        "    from sklearn.metrics import classification_report, confusion_matrix\n",
        "    import numpy as np\n",
        "\n",
        "    predictions = []\n",
        "    for sample, true_label in zip(samples, labels):\n",
        "        spectrogram = audio_to_spectrogram(sample)  # Assume this function is already defined\n",
        "        # Expand dimensions to match the model's expected input\n",
        "        spectrogram = np.expand_dims(np.expand_dims(spectrogram, axis=0), axis=-1)\n",
        "\n",
        "        pred_probs = [model.predict(spectrogram)[0] for model in model_cache.values()]\n",
        "        pred_label = np.argmax(pred_probs)\n",
        "        predictions.append(pred_label)\n",
        "\n",
        "    print(classification_report(labels, predictions, target_names=label_map.values()))\n",
        "    print(confusion_matrix(labels, predictions))\n",
        "\n",
        "samples, true_labels = get_samples_for_each_family()\n",
        "predict_and_analyze(samples, true_labels)\n"
      ],
      "metadata": {
        "id": "QCzmaHxFoWPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noise:\n",
        "To extend the code for these experiments, you'll need to overlay your NSynth dataset samples with different types of background noise (dog barks, traffic noise, nature sounds, and human noise) and then use your pre-trained models to predict the instrument classes in these modified audio samples. This process involves a few steps, including acquiring the noise samples, overlaying these noises onto your test samples, and then conducting predictions with your models.\n",
        "\n",
        "Step 1: Acquire Noise Samples\n",
        "You'll need to find or record a single sample of each noise type. There are many free sound databases online, such as Freesound, where you might find suitable samples. Ensure that any samples you use are free for use and do not require attribution or have copyright restrictions.\n",
        "\n",
        "Step 2: Overlay Noise on Test Samples\n",
        "This step involves mixing your noise sample with each of the test samples from the NSynth dataset. The following is a generic function to overlay noise on an audio sample. This function assumes you've loaded your noise samples into variables such as dog_bark_sample, traffic_noise_sample, nature_noise_sample, and human_noise_sample."
      ],
      "metadata": {
        "id": "MTXKGXykqTKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def overlay_noise(audio_sample, noise_sample, noise_level=0.5):\n",
        "    \"\"\"\n",
        "    Overlays a noise sample onto an audio sample at a specified level.\n",
        "\n",
        "    Parameters:\n",
        "    - audio_sample: The original audio sample.\n",
        "    - noise_sample: The noise sample to overlay.\n",
        "    - noise_level: The volume level of the noise relative to the audio sample.\n",
        "\n",
        "    Returns:\n",
        "    - The audio sample with the noise overlay.\n",
        "    \"\"\"\n",
        "    # Ensure the noise sample is the same length as the audio sample\n",
        "    if len(noise_sample) > len(audio_sample):\n",
        "        noise_sample = noise_sample[:len(audio_sample)]\n",
        "    else:\n",
        "        # Repeat the noise sample if it is shorter than the audio sample\n",
        "        repeat_times = len(audio_sample) // len(noise_sample) + 1\n",
        "        noise_sample = np.tile(noise_sample, repeat_times)[:len(audio_sample)]\n",
        "\n",
        "    # Mix the audio sample with the noise\n",
        "    return audio_sample + noise_level * noise_sample\n"
      ],
      "metadata": {
        "id": "UFZX9d7Vqmg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overlay Noise and Prepare Data\n",
        "First, ensure you have your pre-trained models loaded and accessible in the code. Also, make sure you have your noise samples (dog_bark_sample, traffic_noise_sample, nature_noise_sample, human_noise_sample) ready."
      ],
      "metadata": {
        "id": "abrykR1Jq1rD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example function to load a noise sample - you would replace this with actual loading code\n",
        "def load_noise_sample(file_path):\n",
        "    # This function should load and return the noise sample from the file_path\n",
        "    # For example purposes, this returns a dummy numpy array\n",
        "    return np.random.normal(0, 1, (64000,))\n",
        "\n",
        "# Load your noise samples here\n",
        "dog_bark_sample = load_noise_sample('path/to/dog_bark_sample.wav')\n",
        "traffic_noise_sample = load_noise_sample('path/to/traffic_noise_sample.wav')\n",
        "nature_noise_sample = load_noise_sample('path/to/nature_noise_sample.wav')\n",
        "human_noise_sample = load_noise_sample('path/to/human_noise_sample.wav')\n",
        "\n",
        "def get_test_samples(n_samples=100):\n",
        "    # Define the dataset splits\n",
        "    datasets = {\n",
        "        'train': tfds.load('nsynth/gansynth_subset', split='train', shuffle_files=True),\n",
        "        'valid': tfds.load('nsynth/gansynth_subset', split='validation', shuffle_files=True),\n",
        "        'test': tfds.load('nsynth/gansynth_subset', split='test', shuffle_files=True)\n",
        "    }\n",
        "\n",
        "    samples_per_family = {family: [] for family in label_map.values()}\n",
        "\n",
        "    # Iterate over each dataset split until enough samples per family are collected\n",
        "    for split_name, dataset in datasets.items():\n",
        "        if all(len(samples) >= n_samples for samples in samples_per_family.values()):\n",
        "            break  # Stop if we've already collected enough samples for each family\n",
        "\n",
        "        for example in tfds.as_numpy(dataset):\n",
        "            family_id = example['instrument']['family']\n",
        "            # Convert family_id to instrument name using label_map, skipping if not found (e.g., synth_lead)\n",
        "            instrument_name = label_map.get(family_id)\n",
        "            if instrument_name is None or len(samples_per_family[instrument_name]) >= n_samples:\n",
        "                continue  # Skip if instrument is not recognized or enough samples have been collected\n",
        "\n",
        "            # Convert audio samples to spectrograms and store them with their labels\n",
        "            spectrogram = audio_to_spectrogram(example['audio'])\n",
        "            samples_per_family[instrument_name].append((spectrogram, family_id))\n",
        "\n",
        "    # Aggregate collected samples and labels from all families\n",
        "    aggregated_samples = []\n",
        "    aggregated_labels = []\n",
        "    for family, samples in samples_per_family.items():\n",
        "        for spectrogram, label in samples[:n_samples]:\n",
        "            aggregated_samples.append(spectrogram)\n",
        "            aggregated_labels.append(label)\n",
        "\n",
        "    return np.array(aggregated_samples), np.array(aggregated_labels)\n",
        "\n",
        "# Now, calling this function should give you a balanced dataset\n",
        "test_samples, test_labels = get_test_samples(n_samples=100)\n",
        "\n",
        "# Function to prepare datasets with noise overlay\n",
        "def prepare_dataset_with_noise(test_samples, noise_sample):\n",
        "    noisy_test_samples = []\n",
        "    for sample in test_samples:\n",
        "        noisy_sample = overlay_noise(sample, noise_sample)\n",
        "        noisy_test_samples.append(noisy_sample)\n",
        "    return noisy_test_samples\n",
        "\n",
        "# Prepare datasets for each noise type\n",
        "noisy_datasets = {\n",
        "    \"dog_bark\": prepare_dataset_with_noise(test_samples, dog_bark_sample),\n",
        "    \"traffic_noise\": prepare_dataset_with_noise(test_samples, traffic_noise_sample),\n",
        "    \"nature_noise\": prepare_dataset_with_noise(test_samples, nature_noise_sample),\n",
        "    \"human_noise\": prepare_dataset_with_noise(test_samples, human_noise_sample),\n",
        "}\n"
      ],
      "metadata": {
        "id": "qpn1DVR0rEIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictions and Analysis"
      ],
      "metadata": {
        "id": "O_7jx-qjrGuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(noisy_samples, model_cache):\n",
        "    predictions = []\n",
        "    # Iterate over each sample\n",
        "    for sample in noisy_samples:\n",
        "        spectrogram = audio_to_spectrogram(sample)  # Convert audio sample to spectrogram\n",
        "        spectrogram = np.expand_dims(np.expand_dims(spectrogram, axis=0), axis=-1)  # Reshape for the model\n",
        "\n",
        "        # Aggregate predictions from each model\n",
        "        pred_probs = np.zeros(len(model_cache))\n",
        "        for i, (instrument_name, model) in enumerate(model_cache.items()):\n",
        "            pred = model.predict(spectrogram)[0]\n",
        "            pred_probs[i] = pred\n",
        "\n",
        "        # Determine the predicted class\n",
        "        predicted_class = np.argmax(pred_probs)\n",
        "        predictions.append(predicted_class)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def evaluate_predictions(predictions, true_labels, label_map):\n",
        "    # Convert numerical labels back to class names for a more interpretable report\n",
        "    target_names = [label_map[label] for label in sorted(label_map.keys())]\n",
        "\n",
        "    print(classification_report(true_labels, predictions, target_names=target_names))\n",
        "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
        "    print(conf_matrix)\n",
        "\n",
        "# Assuming `test_labels` is a list of numerical labels corresponding to `label_map`\n",
        "# and `noisy_datasets` is a dictionary with noise type keys and lists of noisy samples as values\n",
        "for noise_type, noisy_samples in noisy_datasets.items():\n",
        "    print(f\"Evaluating with {noise_type} noise...\")\n",
        "    predictions = make_predictions(noisy_samples, model_cache)\n",
        "    evaluate_predictions(predictions, test_labels, label_map)\n"
      ],
      "metadata": {
        "id": "Ggd-ZxPLrRSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "caculation of number of EMR matrix data:"
      ],
      "metadata": {
        "id": "PcVhXXEvoaIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import comb\n",
        "\n",
        "# Redefining the number of samples per class to 10\n",
        "samples_per_class = 10\n",
        "\n",
        "# Solo instruments and combinations\n",
        "solo_samples = 10 * 10  # 10 solo instruments\n",
        "duo_samples = comb(10, 2) * samples_per_class  # Combinations of 2 from 10 instruments\n",
        "trio_samples = comb(10, 3) * samples_per_class  # Combinations of 3 from 10 instruments\n",
        "quartet_samples = comb(10, 4) * samples_per_class  # Combinations of 4 from 10 instruments\n",
        "quintet_samples = comb(10, 5) * samples_per_class  # Combinations of 5 from 10 instruments\n",
        "sextet_samples = comb(10, 6) * samples_per_class  # Combinations of 6 from 10 instruments\n",
        "septet_samples = comb(10, 7) * samples_per_class  # Combinations of 7 from 10 instruments\n",
        "octet_samples = comb(10, 8) * samples_per_class  # Combinations of 8 from 10 instruments\n",
        "nonet_samples = comb(10, 9) * samples_per_class  # Combinations of 9 from 10 instruments\n",
        "all_instruments_samples = samples_per_class  # All 10 instruments together\n",
        "\n",
        "# Adding no instrument samples\n",
        "no_instrument_samples = samples_per_class\n",
        "\n",
        "# Calculating total number of samples with reduced sample size per class\n",
        "total_samples_reduced = (solo_samples + duo_samples + trio_samples + quartet_samples + quintet_samples +\n",
        "                         sextet_samples + septet_samples + octet_samples + nonet_samples + all_instruments_samples +\n",
        "                         no_instrument_samples)\n",
        "\n",
        "total_samples_reduced"
      ],
      "metadata": {
        "id": "oabjOSYd2Td0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow_datasets as tfds\n",
        "import itertools\n",
        "\n",
        "# Define the mapping from numerical labels to string names, excluding 'synth_lead'\n",
        "label_map = {\n",
        "    0: 'bass',\n",
        "    1: 'brass',\n",
        "    2: 'flute',\n",
        "    3: 'guitar',\n",
        "    4: 'keyboard',\n",
        "    5: 'mallet',\n",
        "    6: 'organ',\n",
        "    7: 'reed',\n",
        "    8: 'string',\n",
        "    # 9: 'synth_lead', # Excluded\n",
        "    10: 'vocal',\n",
        "}\n",
        "\n",
        "def audio_to_spectrogram(audio_sample, sr=16000, n_fft=2048, hop_length=512):\n",
        "    \"\"\"Convert audio to spectrogram.\"\"\"\n",
        "    spectrogram = librosa.stft(audio_sample, n_fft=n_fft, hop_length=hop_length)\n",
        "    spectrogram_db = librosa.amplitude_to_db(abs(spectrogram))\n",
        "    return spectrogram_db\n",
        "\n",
        "def mix_audios(audios):\n",
        "    \"\"\"Mix multiple audios into one.\"\"\"\n",
        "    mixed_audio = np.sum(audios, axis=0)\n",
        "    return mixed_audio\n",
        "\n",
        "def load_audio_sample_for_family(family_id, nsynth_dataset):\n",
        "    \"\"\"\n",
        "    Load an audio sample for a specific family from the NSynth dataset.\n",
        "    This is a placeholder function. You need to implement the logic to select and return\n",
        "    an audio sample and its sample rate based on the family_id.\n",
        "    \"\"\"\n",
        "    # Placeholder: Generate a random audio sample\n",
        "    audio_sample = np.random.normal(0, 1, (64000,))  # Assuming 4 seconds of audio at 16kHz\n",
        "    sr = 16000  # Sample rate\n",
        "    return audio_sample, sr\n",
        "\n",
        "def generate_samples_labels(all_sample_size=10):\n",
        "    multiple_validate_sample = []\n",
        "    multiple_validate_label = []\n",
        "\n",
        "   # No instruments scenario\n",
        "    no_instruments_audio = np.random.normal(0, 1, (64000,))  # Assuming 4 seconds of audio at 16kHz\n",
        "    for i in range(all_sample_size):\n",
        "        multiple_validate_sample.append(audio_to_spectrogram(no_instruments_audio))\n",
        "        multiple_validate_label.append([0] * 10)  # No instrument label\n",
        "\n",
        "    # Iterate over each instrument family for solo instruments\n",
        "    for family_id in range(len(label_map)):\n",
        "        for _ in range(all_sample_size):\n",
        "            audio_sample, _ = load_audio_sample_for_family(family_id, nsynth_dataset)\n",
        "            spectrogram = audio_to_spectrogram(audio_sample)\n",
        "            label = [1 if idx == family_id else 0 for idx in range(10)]\n",
        "            multiple_validate_sample.append(spectrogram)\n",
        "            multiple_validate_label.append(label)\n",
        "\n",
        "    # Combinations of instruments\n",
        "    for num_instruments in range(2, len(label_map) + 1):  # From duo to all instruments\n",
        "        for combo in itertools.combinations(range(len(label_map)), num_instruments):\n",
        "            audios = []\n",
        "            labels = [0] * len(label_map)\n",
        "            for family_id in combo:\n",
        "                audio_sample, _ = load_audio_sample_for_family(family_id, nsynth_dataset)\n",
        "                audios.append(audio_sample)\n",
        "                labels[family_id] = 1\n",
        "            mixed_audio = mix_audios(audios)\n",
        "            spectrogram = audio_to_spectrogram(mixed_audio)\n",
        "            multiple_validate_sample.append(spectrogram)\n",
        "            multiple_validate_label.append(labels)\n",
        "\n",
        "    return np.array(multiple_validate_sample), np.array(multiple_validate_label)\n",
        "\n",
        "# Load NSynth dataset\n",
        "nsynth_dataset = tfds.load('nsynth/gansynth_subset', split='test', shuffle_files=True)\n",
        "\n",
        "# Generate samples and labels\n",
        "samples, labels = generate_samples_labels(all_sample_size=10)"
      ],
      "metadata": {
        "id": "1tLjHPD1xC15"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}